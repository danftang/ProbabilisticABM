\documentclass[letterpaper,twocolumn,10pt]{article}
\usepackage{usenix,epsfig,comment,amssymb}
\setlength{\parskip}{1.8mm}
\setlength{\parindent}{0mm}
\begin{document}
%\date{}
\title{\Large \bf Probabilistic Agent Based Modelling: Mathematical foundations}

\author{
{\rm Daniel Tang}\\
Deselby research institute
}

\maketitle

% Use the following at camera-ready time to suppress page numbers.
% Comment it out when you first submit the paper for review.
%\thispagestyle{empty}

\abstract
In order to do modelling well we need to do calibration, data assimilation and uncertainty quantification properly. All of these problems can be posed in terms of probabilistic inference. At present, tools to do probabilistic inference on agent based models are lacking. In this paper I introduce the idea of an agent based dynamical system and develop a mathematical foundation for thinking about and working with probabilistic inference in these systems.

\section{Introduction: The problem}

In this paper we will consider the following problem: Suppose we have a complex system that can be described in terms of interacting agents. Suppose also that we have some (possibly incomplete) knowledge of the behaviour of those agents. Suppose we also have a set of real-world, noisy observations of the aggreagate behaviour of the system as a whole and are able to define an observation operator for each observation type that goes from the state of the system to the observation. We would like to know something about a (hitherto unobserved) observable at some time which may be in the past, present or future, given our knowledge of the behaviour of the agents, our observations and our observation operators. That something may be an expectation value, a modal value, or a distribution chosen from a family that minimises the KL divergence. (or the modal trajectory in time)

\subsection{Defining agents}
We will proceed by specifying our knowledge of the behaviour of an agent as a computer program written from the point of view of an agent. In order to express any incompleteness in our knowledge of the behaviour, we suppose there is a set of global paramaters, which we do not know but which we have a prior distribution for, that may  infulence an agent's behaviour. In addition, we may make the agent's behaviour stochastic in order to express our uncertainty in the agent's behaviour. By doing this, we can track the effect of our uncertainty in the agent's behaviour on the uncertainty in the aggregate behaviour of the system.

\subsection{Defining observation operators}
Similarly, we will define observation operators as computer programs that take a state of the system as input and return an observation (i.e. a real number or vector of reals). The observation operator will itself be stochastic in order to represent the noise in the observation.

We can connect our data to the observation operators by storing the oputput of an observation operator in a variable and allowing an \texttt{observe(x,v)} command which should be interpreted as ``we observe the variable \texttt{x} to have value \texttt{v}''.

In this way, our whole problem can be specified in a single computer program that looks very much like a normal simulation which, if executed, would generate an instance of our data and an instance of the observable we're interested in. The values generated would probably not match our actual data, but there is a positive, if small, probability that they would match.

Our problem can now be cast as one of doing inference on this computer program. That is, finding the expectation value of the observable of interest given that the generated data matches our actual observations.

\section{Agent based dynamical systems}
We introduce the concept of an \textit{agent based dynamical system} which consists of a set of interacting \textit{agents}. Each agent has a state and can interact with other agents. At any given time, $t$, each agent has a propensity, $r$, to perform an action such that, in the infinitesimal slice of time, $[t, t+dt]$, there is a probability $rdt$ that the agent performs the action. The only actions an agent can perform are to remove an agent and to create a new agent. Surprisingly, this will be enough to express all agent behaviours.

\section{Object oriented implementation of an agent based dynamical system}
In order to implement an agent based dynamical system in an object oriented programming language, we have a class, \texttt{ABM}, which is templated on the agent state type \texttt{T}. This class represents a probability distribution over ABM states and has three methods:

\begin{tabular}{|l|p{4.9cm}|}
\hline
\texttt{add(s)} & add an agent in state \texttt{s} to the ABM and return the modified ABM\\
\hline
\texttt{foreach(a)} & execute action \texttt{a} on every agent and return the result\\
\hline
\texttt{observe(o,v)} & observe that the observable \texttt{o} has value \texttt{v} and return the resulting \texttt{ABM}\\
\hline
\end{tabular}

An \texttt{action} is a lambda function that takes an agent state (the state of the agent that is performing the action) and an \texttt{ABM} (the environment the agent finds itself in) and returns an \texttt{ABM} that is the result of performing the action. When passed to a \texttt{foreach} method, the \texttt{ABM} passed to the action is the \texttt{ABM} with the current agent removed.

An \texttt{observable<T>} is a lambda function that takes an \texttt{ABM} and returns an observation of type \texttt{T}.

A \texttt{behaviour} is a set of \texttt{<rate, action>} pairs, where \texttt{rate} is a lambda function that takes an agent state and the \texttt{ABM} and returns a floating point number giving the rate of the behaviour.

Time in the model can be moved forward using the \texttt{Behaviour::integrateTime(abm, Dt)} method, which integrates the abm forward in thim by where \texttt{Dt} units of time and returns the result.


\section{A probabilistic interpretation of an \texttt{ABM}}

A probability distribution over the states of an agent based model is formally a member of a symmetric Fock space on the state of the agents, $T$, which we'll write as $\mathcal{F}(T)$. Surprisingly, we can express everything we need to using just two operators on this space; the \textit{annihilation operator} $a_k$ and the \textit{creation operator} $a_k^\dag$.

The creation operator, $a_k^\dag$, represents the addition of an agent in state $k$ to every state in the probability distribution. By definition, the probability of the ABM being empty after the action of a creation operator is zero.

Creation operators always commute with other creation operators
\begin{equation}
a_k^\dag a_q^\dag = a_q^\dag a_k^\dag
\label{acommute}
\end{equation}

We define the annihilation operator $a_k$ as the operator that has the following properties
\begin{equation}
a_k a_q = a_q a_k
\label{ccommute}
\end{equation}
\begin{equation}
a_k a_q^\dag - a_q^\dag a_k = \delta_{kq}
\label{symcommute}
\end{equation}
\begin{equation}
a_k\mathbf{1} = \mathbf{0}
\label{adagzero} 
\end{equation}
where $\delta_{kq}$ is the Kronecker delta function and we use $\mathbf{1}$ to represent the ABM probability distribution representing an empty ABM with probability 1 and $\mathbf{0}$ to represent the distribution with all states having zero probability.

For a given (non-probabilistic) ABM, we define the \textit{occupation number} of agent state $s$ to be the number of agents in the ABM that are in that state. The annihilation operator $a_s$ multiplies the probability of each ABM state by the occupation number of $s$ and removes one agent in state $s$.

We can see that this behaviour follows from the properties above using a recursive proof. Suppose that $a_k a_k^\dag S = nS$ then, from equation \ref{symcommute}
\[
a_k a_k^\dag a_k^\dag S = (1 + a_k^\dag a_k)a_k^\dag S = (n+1)a_k^\dag S
\]
We can start the recursion using equations \ref{symcommute} and \ref{adagzero}
\[
a_k a_k^\dag \mathbf{1} = (1 + a_k^\dag a_k)\mathbf{1} = \mathbf{1}
\]

\subsection{Single agent}
We start with the simplest possible agent based dynamical system: a system which is either empty or there is a single, stateless agent. Suppose that at time $t=0$ the agent is definitely present and between time $t$ and $t+dt$ the agent leaves the system with probability $rdt$. Here's the program that represents this system
\begin{verbatim}
  abm = ABM()
  abm.add(void)
  
  b = Behaviour()
  b.add({1.0}, (state, abm) -> {
    return abm
  })
  return b.integrateTime(abm, T)
\end{verbatim}
remember that the ABM passed to the action in the bahaviour is the ABM minus the agent whose behaviour we're considering. So in this case, it's the empty ABM and returning this is equivalent to the agent removing itself from the system.

Starting from the first line of the program, we create a new ABM which is definitely empty, so is in the state $\mathbf{1}$. We then add an agent, after which the ABM will be in state $a^\dag \mathbf{1}$. Next we create a behaviour. The action is just the ``do nothing'' action, which we can think of as the identity operator, $1$. We'll think of a behaviour as an operator that takes a probability distribution over ABMs and returns a rate of change of that probability distribution.

----------------------- TODO -------------

 Given an empty state and an agent that is currently outside of the system that has a propensity, $r$, to enter the system, then in time slice $[t, t+dt]$ there is a probability $rdt$ that the state will switch from empty to having an agent in it. So in time $dt$ the probability of the empty state goes down by $rdt$ and the probability of there being an agent goes up by $rdt$. So the rate of change of the state is givcen by
\[
H_{a^\dag} = \frac{\partial}{dt} = (a^\dag - 1)r
\]
Similarly, for the annihilation operator. If we start with an agent in the system, the probability of there being an agent decreases by $rdt$ while the probability of the state being empty increases by the same amount. We can achieve this with the operator
\[
H_a = \frac{\partial}{\partial t} = (a - a^{\dag}a)r
\]
We'll call $H_{a\dag}$ and $H_a$ the \textit{Hamiltonians} for their respective operators. Again, we'll see later that by choosing this form for the Hamiltonians we make things very elegant when we come to generalise to many agents.

Once we have the Hamiltonian for a probability distribution, the time evolution of that probability distribution is completely defined by the equation
\[
\frac{\partial P}{\partial t} = HP
\]
which is the probabilistic dynamics' analogue of Schr\"{o}dinger's equation. This equation has a very general solution
\[
P(t) = e^{tH}P(0)
\]
where the exponent of an operator is defined as
\[
e^{tH} = \sum_{k=0}^\infty \frac{(tH)^k}{k!}
\]
and eponentiation of an operator is just repeated application (taking care to respect the order of non-commutative operators).

Going back now to the agent that has propensity $r$ to annihilate itself, the equation of motion is
\[
\frac{\partial P}{\partial t} = H_aP = \left(\frac{\partial }{\partial a^\dag} - a^\dag\frac{\partial }{\partial a^\dag}\right)rP
\]
Solving this for $P(0) = a^\dag$ gives
\[
P(t) = e^{rt((1-a^\dag)\frac{\partial}{\partial a^\dag})}a^\dag
\]
\[
 = \sum_{k=0}^\infty \frac{\left(rt(1-a^\dag)\frac{\partial}{\partial a^\dag}\right)^k}{k!}a^\dag
\]
\[
= \sum_{k=0}^\infty \frac{(-rt)^k\left((a^\dag-1)\frac{\partial}{\partial a^\dag}\right)^k}{k!}a^\dag
\]
but since $\frac{\partial^n}{\partial a^{\dag n}} = 0$ for $n>1$
\[
\left((a^\dag-1)\frac{\partial}{\partial a^\dag}\right)^k = (a^\dag-1)\frac{\partial}{\partial a^\dag}
\]
for $k>0$, so
\[
P(t) = 1 + \sum_{k=0}^\infty \frac{(-rt)^k}{k!}(a^\dag-1)
\]
where the initial $1$ accounts for the case when $k=0$. We can now turn the sum back into an exponent to give
\[
P(t) = (1 - e^{-rt}) + e^{-rt}a^\dag
\]
So the probability that the agent exists reduces exponentially as time progresses. This is exactly what we would expect.

\subsection{Many agents}

We can now show how our operators extend very elegantly to systems with more than one agent. In this case, we represent the state that there are definitely $n$ agents in the system as $P = a^{\dag n}$. A probability distribution for a set of agents can therefore represented by a polynomial
\[
P = \sum_{n=0}^\infty p_n a^{\dag n}
\]
where $p_n$ is the probability that there are $n$ agents, and
\[
\sum_{n=0}^\infty p_n = 1
\]
By choosing this representation, we can see immediately that our creation operator works on definite states:
\[
a^\dag a^{\dag n} = a^{\dag n+1}
\]
Even better, it works on probabilistic states too. For example, suppose there's a $0.5$ probability that the system is empty and a $0.5$ probability that there's one agent, so $P = 0.5 + 0.5a^\dag$. In this case
\[
a^\dag (0.5 + 0.5a^\dag) = 0.5a^\dag + 0.5a^{\dag 2}
\]
So, after applying the creation operator, there's an equal probability of there being either one or two agents, i.e. the operator simultaneously adds an agent to all possible states.

If we start with an empty system (i.e. $P(0) = 1$) and add agents at a constant rate we'd expect the Hamiltonian to be given by
\[
\frac{\partial P}{\partial t} = H_{a\dag}P = (a^\dag - 1)rP
\]
This can again be solved analytically:
\[
P(t) = e^{tH} = e^{-rt(1-a^\dag)} = e^{-rt}e^{rta^\dag}
\]
\[
= e^{-rt}\sum_k \frac{(rt)^k}{k!}a^{\dag k}
\]
so, the probability of there being $k$ agents (i.e. the coefficient of the $k^{th}$ power of $a^\dag$) is 
\[
c_k = \frac{(rt)^ke^{-rt}}{k!}
\]
which is just the Poisson distribution, as we would expect.

Now let's try the annihilation operator on $P = 0.5a^\dag + 0.5a^{\dag 2}$:
\[
a (0.5a^\dag + 0.5a^{\dag 2}) = 0.5 + a^\dag
\]
We don't get $0.5 + 0.5a^\dag$ as you may have expected. Instead, for a distribution $P = \sum_n p_na^{\dag n}$ we get
\[
a \sum_n p_na^{\dag n} = \sum_n n p_n a^{\dag n-1}
\]
which isn't even a valid probability distribution. Howerver, this behaviour can be harnessed to make a very useful operator
\[
N = a^\dag a
\]
which in quantum field theory is called the number operator because it multiplies the probability of each state by the number of agents in that state
\[
a^\dag a \sum_n p_na^{\dag n} = \sum_n n p_n a^{\dag n}
\]
We can now understand the Hamiltonian of the annihilation operator in a new light
\[
H_a = (a - a^\dag a)r = (a^{\dag -1} - 1)Nr
\]
where $a^{\dag -1}$ is the inverse of the creation operator, which we'll call the agent-based annihilation operator. In this form, we can understand the $a^{\dag -1}$ term as the transition operator for a single agent's propensity. The number operator $N$ has the effect of applying this operator to all the agents in the system, as if we'd executed a \texttt{foreach} loop in a computer program.

This works for any operator that represents the transition function for an agent's propensity. Because it's so useful, well define a higher order function which takes an agent action $T$ and returns the Hamiltonian that applies this propensity to all agents in a probabilistic state
\[
\mathcal{H}_r(T) = (T - 1)Nr
\]
we'll call this the \textit{foreach} operator.

To illustrate this, suppose we start with $n$ agents and each agent has a propensity, $r$, to annihilate itself. In pseudocode this may look something like
\begin{verbatim}
foreach(a in S) {
  if(random() < r*dt) delete(a)
}
\end{verbatim}

We can create the Hamiltonian for this
\[
\frac{\partial}{\partial t} = \mathcal{H}(a^{\dag -1}) = (a^{\dag-1} - 1)Nr
\]
Expanding this gives
\[
H = (a^{\dag-1} - 1)a^\dag ar = (a - a^\dag a)r
\]
which is exactly the Hamiltonian from the single agent case, but now we understand why it has this form and why it will also work in the case of many agents.

The only difference here is the boundary condition. Instead of $S(0) = a^\dag$ as in the single agent case, we now have $P(0) = a^{\dag n}$. The solution to this is
\[
P(t) = \sum_m{n\choose m}e^{-rtm}(1-e^{-rt})^{n-m}a^{\dag m}
\]
The coefficients of $a^\dag$ (i.e. the probabilities that there are $m$ agents) make up a Binomial distribution, which, again, is what we would expect.

The above examples, although very simple, demonstrate how we can define a Hamiltonian operator in terms of the behaviour of a single agent then apply it to each agent in a system and ultimately describe the aggregate dynamics of a probability distribution over the whole system.



\subsection{Agent states}

So far, the agents we've been considering haven't had any internal state. We'll start simple again by giving agents a binary state which can be either $0$ or $1$. We'll represent an agent in state $0$ as $a^\dag_0$ and an agent in state $1$ as $a^\dag_1$. A system with $n$ agents in state $a^\dag_0$ and $m$ agents in state $a^\dag_1$ will be represented as $a^{\dag n}_0 a^{\dag m}_1$, so a probability distribution over all states is again a polynomial, but this time in two variables. We define annihilation operators for each agent state:
\[
a_0 = \frac{\partial}{\partial a^\dag_0}
\]
\[
a_1 = \frac{\partial}{\partial a^\dag_1}
\]
This induces two number operators $N_0 = a_0^\dag a_0$ and $N_1 = a_1^\dag a_1$ and two foreach operators $\mathcal{H}_{0r}(T) = (T-1)N_0r$ and $\mathcal{H}_{1r} = (T-1)N_1r$ which loop over agents in state $0$ and $1$ respectively.

A change of state from $x$ to $y$ can be represented as an agent in state $x$ annihilating itself while simultaneously creating a new agent in state $y$, $a_y^\dag a_x^{\dag-1}$.

Let's illustrate this with a system of $n$ agents who each have a propensity $r$ to flip to the opposite state. In pseudocode we want something like
\begin{verbatim}
S0  = S
foreach(x in S0 suchthat S0.state(x)==0) {
  if(random() < r*dt) S.state(x) = 1
}
foreach(x in S0 suchthat S0.state(x)==1) {
  if(random() < r*dt) S.state(x) = 0
}
\end{verbatim}
 This can be expressed as a Hamiltonian
\[
H = r\mathcal{H}_{0r}(a^\dag_1 a^{\dag -1}_0) + r\mathcal{H}_{1r}(a^\dag_0 a^{\dag -1}_1)
\]
expanding this gives
\[
H = r\left((a^\dag_1 - a^\dag_0)\frac{\partial}{\partial a^\dag_0} + (a^\dag_0 - a^\dag_1)\frac{\partial}{\partial a^\dag_1}\right)
\]
If we suppose that at time $t = 0$ all agents are in state $a^\dag_1$ then we can solve the governing equation $\frac{\partial P}{\partial t} = HP$ to give
\[
P(t) = 2^{-n} \sum_{m=0}^n {n\choose m} (1-e^{-2rt})^{n-m}(1+e^{-2rt})^m a^{\dag n-m}_0 a^{\dag m}_1
\]
[TODO: add proof]

We can represent an agent with more than one variable in it's state quite naturally by allowing vectors in the subscripts of $a^\dag$. So, for example, an agent with two binary values, $a$ and $b$ could be referred to as $a^\dag_{[a,b]}$.

\subsection{Interacting agents}

Now let's get agents to interact. Suppose we have a conpartmental model of infectious disease spread. Suppose there's a population of $n$ people. Each person can be one of suseptible, infected or recovered. Infected people have a propensity, $\beta$, to meet and infect a suseptible person, in which case the suseptible person becomes infected. Also, infected people have a propensity, $\gamma$ to recover.

In order to represent the interaction of infected people with suseptible, we want a Hamiltonian that would be equivalent to pseudocode something like
\begin{verbatim}
foreach(x in S0 suchthat 
        S0.state(x)==Suseptible) {
  foreach(y in S0 suchthat
          S0.state(y) == Infected) {
    if(random() < beta*dt)
      S.state(x) = Infected
  }
}
\end{verbatim}

It turns out that the number operator can be used in a quite natural way to represent this nested \texttt{foreach} operator.

If we let $a^\dag_S$, $a^\dag_I$ and $a^\dag_R$ represent suseptible, infected and recovered people respectively, then the Hamiltonian we're after is just
\[
H = \mathcal{H}_{S\beta}(a^\dag_Ia^{\dag-1}_SN_I) = (a^\dag_Ia^{\dag-1}_SN_I - 1)N_S\beta
\]
so, the Hamiltonian for the whole model is just
\[
H = \mathcal{H}_{S\beta}(a^\dag_Ia^{\dag-1}_SN_I) + \mathcal{H}_{I\gamma}(a^\dag_Ra^{\dag-1}_I)
\]
and the equation of motion for the probability distribution over states is, as always
\[
\frac{\partial P}{\partial t} = HP
\]
Although this is a differential equation, it's worth pointing out explicitly that it is representing discrete interactions between agents. So, it captures things like disease eradication when the number of infected agents is small, unlike the standard differential version of the SIR model, which is only accurate for large numbers of agents.

Notice also that the different number operators are able to single out agents based on their state. If we allow higher-order operators, we can extend this idea to any predicate on the state of an agent, so that we can loop over any describable subset of agents. For example, suppose agents are located on a 2D grid and each agent has a state $\mathbf{v} = (x,y)$ giving the agent's position on that grid. We could express the program
\begin{verbatim}
foreach(a in S0) {
  foreach(b in S0 suchthat 
    norm(a.v-b.v)<4) {
    call F(a.v,b.v)
  }
}
\end{verbatim}
as
\[
H = \sum_{(a_v\in S)}\sum_{(b_v\in \{v | |a_v-v| < 4\})} \mathcal{H}_{a_v}(F(a_v,b_v)N_{b_v})
\]
\subsection{Arithmetic}

If an agent has numerical variables in its state, we can represent arithmetic on those variables with a change of state operation. For example, if an agent has two integers, $a$ and $b$ in its state, we can represent the operation $a = a + b$ as
\[
a^\dag_{[a+b,b]}a^{\dag-1}_{[a,b]}
\]
and so the Hamiltonian for an agent that has a unit propensity to perform this operation is
\[
H = \mathcal{H}_{[a,b]}(a^\dag_{[a+b,b]}a^{\dag-1}_{[a,b]}) = (a^\dag_{[a+b,b]}a^{\dag-1}_{[a,b]}-1)N_{[a,b]}
\]
The same method can be used to apply any operator on any number of variables.

\subsection{Continuous states and the uncertainty principle}

So far, we've only considered agents with a finite number of discrete states. We may want to endow the agent with a real number as part of its state, in which case the agent can be in any one of an uncountable infinity of states and we'd need a polynomial in an infinite number of variables to represent this.

The idea of using a polynomial to represent a probability distribution carries over to the infinitesimal case quite naturally. The transition is made easier if we think of the coefficients as a function from monomials to coefficient values. For example, for a monovariate polynomial
\[
P = \sum_n c_n a^{\dag n} = \sum_n f(a^{\dag n})a^{\dag n} 
\]
In this format we can go over to the infinitesimal limit by replacing the monomials with sets of real numbers. The coefficients can now be represented as a function from sets of real numbers to probability densities (i.e. a PDF over $\mathcal{P}(\mathbb{R})$, the power set of the real numbers). The probability distribution now becomes
\[
P = \int_{\mathbf{z} \in \mathcal{P}(\mathbb{R})} f(\mathbf{z})
\]
Note that because we chose sets rather than bags to represent monomials, we can now no longer represent multiple agents in the same state, but because each state is infinitesimally small the probability of finding more than one agent in a state is of second order.

Multiplication of a state by $\mathbf{z}_n = \left\{a^\dag_n\right\}$ becomes
\[
\mathbf{z_n}P \equiv \int_{\mathbf{z} \in \mathcal{P}(\mathbb{R})} f(\mathbf{z} \setminus\mathbf{z_n})
\]
while differentiation becomes
\[
\frac{\partial P}{\partial \mathbf{z_n}}  \equiv \int_{\mathbf{z} \in \mathcal{P}(\mathbb{R})} f(\mathbf{z} \cup \left\{a^\dag_n\right\})
\]
[TODO: make these integrals more formally correct. Define as differential ring/algebra?]

Consider an agent with a 1-dimensional, real valued state. We'd naturally represent a probability distribution over this value as a density function $P:\mathbb{R} \rightarrow \mathbb{R}$. For such a density function, we define the information of the function to be
\[
I = \int_{-\infty}^\infty P(x) \log(P(x)) dx
\]
We can escape the infinitude of variables by noting that in all realistic scenarios, the amount of information we have about an agent is finite. That is, whatever observations we make of a variable, there always remains some inherent uncertainty. This is analogous to Heisenberg's uncertainty principle: the more precicely we measure the location of a particle, the more momentum it must have. In our case, momentum is replaced with information and we are only interested in cases of finite information.

Given this, we can represent any probability density that has finite information as a probability mass function over a finite number of variables. For example, if the PDF has a compact support, a convenient representation would be as a Bernstein polynomial, a Deselby polynomial or the spectral modes.

[TODO: find good bases that make the mapping simple from agent to aggregate behaviour]

\subsection{Mixed real/integer states}
 We can create a fully general system state that can deal with mixed real and integer agent states by defining the state to be a function from bags of state vectors to real coefficients.
 \[
 P = \int_{\Psi\in\mathcal{P}(\psi)} f(\Psi)
 \]

\section{Making observations and doing inference}

If we have a prior $P(S)$ over system states and an observation operator $O$ that goes from a probability distribution over a system state to a probability distribution over the reals and we observe that the obervation operator has value $o$ then we can define the posterior
\[
P(S|OS = o) \propto P(OS=o|S)P(S)
\]
i,e, we multiply the prior probability distribution over $S$ by the likelihood of the observation. We can represent likelihoods as polynomials in a similar way to probability distributions
\[
P(o|.) = \sum_n P(o|a^{\dag n})a^{\dag n}
\]
However, multiplication of two probability distributions is not the same as polynomial multiplication. Instead, it is multiplication of the coefficients:
\[
\left(\sum_n c_n a^{\dag n}\right) \times \left(\sum_m d_m a^{\dag m}\right) = \sum_n c_nd_n a^{\dag n}
\]

Going back to the example of a single agent that has a unit propensity to annihilate itself. If we start with a prior that there's a 0.5 probability that the agent exists, then observe that it doesn't exist at time $t$, what's the posterior probability that it existed at time $t=0$? The equation of motion is
\[
\frac{\partial P}{\partial t} = HP = \left(\frac{\partial}{\partial a^\dag} - a^\dag \frac{\partial }{\partial a^\dag}\right)P
\]
Integrating this for the case $P(0) = (1-p) + pa^\dag$
\[
P(t) = e^{\frac{\partial}{\partial a^\dag} - a^\dag \frac{\partial }{\partial a^\dag}}S
\]
\[
= 1-pe^{-rt} + e^{-rt}pa^\dag
\]
So,
\[
P(o|.) = 1 + (1-e^{-rt})a^\dag
\]
So
\[
P(.|o) \propto (1-p) + (1-e^{-rt})pa^\dag
\]
normalising
\[
\int P(.|o) dp = (1-0.5e^{-rt})
\]
\[
P(.|o) = \frac{(1-p) + (1-e^{-rt})pa^\dag}{(1-0.5e^{-rt})}
\]
So, if the prior $P(0) = 0.5 + 0.5a^\dag$ then the posterior is given by
\[
P(.|o) = \frac{0.5 + 0.5(1-e^{-rt})a^\dag}{(1-0.5e^{-rt})}
\]

\section{Numerical methods}

So far, we've considered simple systems that have analytic solutions. However, almost all of the systems of interest will not have an analytic solution. We'll now introduce some techniques to help us to numerically approximate solutions.

Because the number of coefficients in the probabilistic state of a system expands exponentially with the number of variables in an agent's state and with the number of agents, for most real world models it isn't feasible to explicitly represent the probabilistic state as a list of coefficients. There are three ways around this. We can represent the state symbolically, we can approximate the state or we can abstract over the state.

\subsection{Symbolic representation of probability distributions}

In almost all cases of interest a probability distribution, although very high dimensional and complex, will be the result of a (potentially large but manageable) number of operations. What's more, we can construct these states from the empty set of agents $P=1$. So, instead of representing a system state as a list of monomial coefficients we can represent it as a list of operations on $1$:
\[
P = \prod_n O_n1
\]
all of which are constructed from the creation and annihilation operators.

When thought of like this, we never have to explicitly represent a probability distribution as a polynomial or as any other kind of representation. Instead, all we ever need to deal with is creation and annihilation operators on probability distributions.

We can reduce the necessary behaviour of these operators to an abstract algebra that obeys the following axiomatic rules:
\begin{equation}
a^\dag 0 = 0
\label{zerozero}
\end{equation}
where $d_{kq}$ is the Kronecker delta function.

Using these rules, we can show that everything acts as expected. Starting with the number operator, $a^\dag a$, we have, from equations \ref{adagzero} and \ref{zerozero}
\[
a^\dag a 1 = 0
\]
and more generally, if $a^\dag a S = nS$ then, from equation \ref{symcommute}
\[
a^\dag a a^\dag S = a^\dag(a^\dag a +1)S = (n+1)a^\dag S
\]
so the number operator acts as expected.

Given this we can show that the annihilation operator also acts as expected. Suppose again that $a^\dag a S = nS$ then
\[
a a^\dag S = (a^\dag a+1)S = (n+1)S
\]

Equations \ref{ccommute} and \ref{acommute} ensure that these results extend to multivariate cases.

\subsection{State approximation: Projecting to lower dimensional spaces}

We can also often approximate a high-dimensional probabilistic state in a lower dimensional space that is more practical to work with.

\subsection{State abstraction: Projection to lower dimensional spaces without approximation}


\section{Integrating over propensities: The path integral}

\section{Data assimilation}

\begin{comment}
\section{ABM as a computational paradigm}

We define an ABM as a set of (Turing complete) computational processes, which we call \textit{agents}. Without loss of generality, we assume that each agent is executing the same program, although their internal states may differ. In addition, there is an \textit{environment} which has a finite state and a \textit{public API} through which any agent may perform actions or query for information. Similarly, the each agent has a public API through which the environment may act on the agent or gain information about the state of the agent. The environment can also create or annihilate agents.

Computation proceeds in three modes
\begin{description}

\item[Synchronous mode] In which each agent gets to perform computations and act on the environment through repeated execution of a \textit{timestep} function. For each timestep of the whole model, the state of the environment is only updated once per timestep.

\item[Sequential mode] In which each agent has a unique identity which has an ordering. Computation proceeds through repeated execution of each agent's timestep function \textbf{in order of their identity}. This time the environment is updated immediately as agents act upon it.

\item[Asymchronous mode] In which each agent is a separate process which communicates with the environment through asynchronous message passing. We will not consider this mode in this paper.

\end{description}

It is immediately clear that an ABM is a Turing complete computer, so can be considered to be a paradigm for computation. This paradigm is particularly convenient when we are modelling domains that we would naturally think in terms of multiple, interacting agents.

Since both sequential and synchronous modes are Turing complete, it follows that they are equivalent. A constructive proof of this is as follows: A synchronous mode ABM can calculate an sequential mode ABM by having the environment assignin a unique ID to each agent. At each timestep the environment passes a \textit{right of execution} to a single agent. Upon execution of an agents timestep function, the agent may only act if it is currently holding a \textit{right of execution}. Once used, the agent passes the right of execution back to the environment, which then passes it on to the next agent in the ordering of IDs. Similarly, a sequential mode ABM can calculate a synchronous mode ABM by having each agent hold a boolean state which specifies whether the next timestep is a calculation step or an action step. On the calculation step, each agent calculates its actions for that step, while on an action step each agent performs the pre-caclulated action. The boolean state is flipped at the end of each step. Since, in synchronous mode, agent's actions must commute the resulting computation is independent of the ordering of the agents.

Given this equivalence, we will consider only synchronous mode ABMs in the remainder of this paper.

\section{Probabilistic semantics for ABMs}

\subsection{Fixed number of agents, no interaction}

Let's start with a vert simple ABM: an agent-based simulation of a Markov process.  

The state of an ABM can be defined as a pair $(E ,\Psi)$, where $E$ is the state of the environment and $\Psi$ is a bag of states of the agents\footnote{where a \textit{bag} is like a set but it may contain multiple members with the same state.}.
\end{comment}


%{\footnotesize \bibliographystyle{acm}
%\bibliography{sample}}


%\theendnotes

\end{document}
